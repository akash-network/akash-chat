export interface Model {
  id: string;
  name: string;
  description?: string;
  available: boolean;
  temperature?: number;
  top_p?: number;
  tokenLimit?: number;
  owned_by?: string;
  parameters?: string;
  architecture?: string;
  hf_repo?: string;
  aboutContent?: string;
  infoContent?: string;
  thumbnailId?: string;
  deployUrl?: string;
}

export const models: Model[] = [
  {
    id: 'DeepSeek-R1-0528',
    name: 'DeepSeek R1 0528',
    description: 'Strong Mixture-of-Experts (MoE) LLM',
    available: true,
    temperature: 0.6,
    top_p: 0.95,
    tokenLimit: 64000,
    parameters: '671B',
    architecture: 'Mixture-of-Experts',
    hf_repo: 'deepseek-ai/DeepSeek-R1-0528',
    aboutContent: `Experience **DeepSeek R1 0528**, the latest iteration of DeepSeek's groundbreaking reasoning model. This advanced 671B parameter Mixture-of-Experts (MoE) architecture represents a significant leap forward in AI reasoning capabilities, featuring enhanced chain-of-thought processing and superior problem-solving abilities.

The 0528 version introduces refined training techniques and improved reasoning pathways, making it exceptionally powerful for complex analytical tasks, mathematical reasoning, and multi-step problem solving. Built for professionals who demand the highest level of AI performance.`,
    infoContent: `
* ‚ö° Latest DeepSeek R1 0528 with enhanced reasoning capabilities
* üß† Advanced chain-of-thought processing with 671B parameters
* üåê Decentralized hosting for cost-effective, unrestricted access
* üîç Optimized for complex reasoning, analysis, and problem-solving tasks`,
    thumbnailId: 'deepseek',
    deployUrl: 'https://console.akash.network/templates/akash-network-awesome-akash-DeepSeek-R1-0528'
  },
  {
    id: 'Qwen3-235B-A22B-FP8',
    name: 'Qwen3 235B A22B',
    description: 'Advanced reasoning model with 235B parameters (22B active)',
    available: true,
    temperature: 0.6,
    top_p: 0.95,
    tokenLimit: 128000,
    parameters: '235B (22B active)',
    architecture: 'Mixture-of-Experts (128 experts)',
    hf_repo: 'Qwen/Qwen3-235B-A22B-FP8',
    aboutContent: `Experience the power of **Qwen3 235B A22B**, a cutting-edge Mixture-of-Experts model with 235B total parameters and 22B active parameters. This advanced model excels in reasoning, instruction-following, and multilingual support, offering seamless switching between thinking and non-thinking modes.

Qwen3 235B A22B delivers superior performance in complex logical reasoning, mathematics, coding, and creative writing, making it ideal for demanding AI applications.`,
    infoContent: `
* ‚ö° Instant access to Qwen3 235B A22B with no signup
* üß† Supports up to 32K tokens with YaRN scaling up to 131K
* üåê Decentralized hosting for lower costs & full control
* üîç Optimized for reasoning, coding, and multilingual tasks`,
    thumbnailId: 'llama-3',
    deployUrl: 'https://console.akash.network/templates/akash-network-awesome-akash-Qwen3-235B-A22B-FP8'
  },
  {
    id: 'meta-llama-Llama-4-Maverick-17B-128E-Instruct-FP8',
    name: 'Llama 4 Maverick 17B 128E',
    description: '400B parameter model (17B active) with 128 experts',
    available: true,
    temperature: 0.6,
    top_p: 0.9,
    tokenLimit: 128000,
    parameters: '400B',
    architecture: 'Mixture-of-Experts (128 experts)',
    hf_repo: 'meta-llama/Llama-4-Maverick-17B-128E-Instruct',
    aboutContent: `Looking to explore Meta's Llama 4 Maverick 17B 128E? AkashChat lets you experience this cutting-edge multimodal language model in real time‚Äîno setup required. Powered by a Mixture-of-Experts (MoE) architecture with 128 experts and 17B active parameters per pass, Maverick delivers top-tier performance in reasoning, coding, and multimodal tasks.

AkashChat provides a fast, user-friendly interface to chat with Llama 4 Maverick‚Äîleveraging decentralized compute on the Akash Network.`,
    infoContent: `
* ‚ö° Instant access to Llama 4 Maverick with no signup
* üß† Run on a 1M-token context window with advanced multimodal capabilities
* üåê Decentralized hosting for lower costs & full control
* üîç Optimized for developers, researchers, and AI enthusiasts`,
    thumbnailId: 'llama-4',
    deployUrl: 'https://console.akash.network/templates/akash-network-awesome-akash-Llama-4-Maverick-17B-128E-Instruct-FP8'
  },
  {
    id: 'nvidia-Llama-3-3-Nemotron-Super-49B-v1',
    name: 'Llama 3.3 Nemotron Super 49B',
    description: 'Great tradeoff between model accuracy and efficiency',
    available: true,
    temperature: 0.6,
    top_p: 0.95,
    tokenLimit: 128000,
    parameters: '49B',
    architecture: 'Optimized Transformer',
    hf_repo: 'nvidia/Llama-3.3-Nemotron-Super-49B-v1',
    aboutContent: `Experience **Llama 3.3 Nemotron Super 49B**‚Äîa powerful open-source model that strikes the perfect balance between performance and efficiency. Available now on AkashChat, this high-capacity model delivers excellent results in reasoning, generation, and coding tasks without sacrificing speed.

Powered by NVIDIA's cutting-edge design, Nemotron Super 49B is perfect for developers and researchers looking to maximize output on a flexible, decentralized platform.`,
    infoContent: `
* ‚ö° Instant access to Llama 3.3 Nemotron Super 49B with no signup
* üß† Supports massive 128K-token context for long-form content  
* üåê Decentralized hosting for lower costs & full control
* üîç Optimized for developers, researchers, and AI enthusiasts`,
    thumbnailId: 'llama-1',
    deployUrl: 'https://console.akash.network/templates/akash-network-awesome-akash-Llama-3.3-Nemotron-Super-49B-v1'
  },
  {
    id: 'Qwen-QwQ-32B',
    name: 'Qwen QwQ-32B',
    description: 'Medium-sized reasoning model with enhanced performance',
    available: true,
    temperature: 0.6,
    top_p: 0.95,
    tokenLimit: 128000,
    parameters: '32B',
    architecture: 'Reasoning-optimized',
    hf_repo: 'Qwen/QwQ-32B',
    aboutContent: `Unlock the capabilities of **Qwen QwQ-32B**, a versatile reasoning model optimized for both general-purpose and complex tasks. On AkashChat, you get instant access to this medium-sized powerhouse‚Äîno setup required.

Qwen QwQ-32B blends fast inference with high accuracy, making it ideal for researchers, developers, and creators pushing the boundaries of LLM capabilities.`,
    infoContent: `
* ‚ö° Lightning-fast access with no login  
* üß† Long context window (128K tokens) for better coherence 
* üåê Decentralized hosting for lower costs & full control
* üîç Great for logic, Q&A, and creative content generation`,
    thumbnailId: 'llama-3',
    deployUrl: 'https://console.akash.network/templates/akash-network-awesome-akash-QwQ-32B'
  },
  {
    id: 'Meta-Llama-3-3-70B-Instruct',
    name: 'Llama 3.3 70B',
    description: 'Well-rounded model with strong capabilities',
    available: true,
    temperature: 0.6,
    top_p: 0.9,
    tokenLimit: 128000,
    parameters: '70B',
    architecture: 'Transformer',
    hf_repo: 'meta-llama/Llama-3.3-70B-Instruct',
    aboutContent: `Meet **Llama 3.3 70B**, Meta's well-rounded large model available now on AkashChat for instant access. With strong performance across tasks‚Äîreasoning, summarization, coding‚Äîthis model is a reliable all-rounder for both casual and professional users.

Enjoy top-tier performance and low-latency interaction without needing to configure anything.`,
    infoContent: `
* ‚ö° Jump in with zero setup  
* üß† Handles long conversations with a 128K token limit  
* üåê Cost-effective, censorship-resistant hosting  
* üîç Ideal for devs, startups, and AI researchers`,
    thumbnailId: 'llama-2',
    deployUrl: 'https://console.akash.network/templates/akash-network-awesome-akash-Llama-3.3-70B-Instruct'
  },
  {
    id: 'DeepSeek-R1',
    name: 'DeepSeek R1 671B',
    description: 'Strong Mixture-of-Experts (MoE) LLM',
    available: true,
    temperature: 0.6,
    top_p: 0.95,
    tokenLimit: 64000,
    parameters: '671B',
    architecture: 'Mixture-of-Experts',
    hf_repo: 'deepseek-ai/DeepSeek-R1',
    aboutContent: `Tap into the strength of **DeepSeek R1 671B**, one of the most capable Mixture-of-Experts (MoE) models available. Now live on AkashChat, this massive model offers world-class performance on tasks like reasoning, planning, and instruction-following.

Built to scale, DeepSeek R1 uses expert routing to reduce compute while maximizing results‚Äîideal for large-scale AI development.`,
    infoContent: `
* ‚ö° Instant access to DeepSeek R1 671B with no signup
* üß† Efficient MoE architecture with scalable performance 
* üåê Decentralized hosting for lower costs & full control
* üîç Built for professionals tackling high-complexity tasks`,
    thumbnailId: 'deepseek',
    deployUrl: 'https://console.akash.network/templates/akash-network-awesome-akash-DeepSeek-R1'
  },
  {
    id: 'Meta-Llama-3-1-405B-Instruct-FP8',
    name: 'Llama 3.1 405B',
    description: 'Most capable model for complex tasks',
    available: true,
    temperature: 0.6,
    top_p: 0.9,
    tokenLimit: 60000,
    parameters: '405B',
    architecture: 'Transformer',
    hf_repo: 'meta-llama/Llama-3.1-405B-Instruct-FP8',
    aboutContent: `Explore the high-performance **Llama 3.1 405B**, Meta's most capable model for complex reasoning, code generation, and advanced natural language tasks. Live on AkashChat, you can access this model instantly‚Äîno hardware required.

With 405 billion parameters, this model excels at deep understanding, long-context retention, and high-quality generation for even the most demanding workloads.`,
    infoContent: `
* ‚ö° Instant access to Llama 3.1 405B with no signup 
* üß† Handles long conversations with a 128K token limit  
* üåê Cost-effective, censorship-resistant hosting  
* üîç Best for complex enterprise-grade AI tasks`,
    thumbnailId: 'llama-3',
    deployUrl: 'https://console.akash.network/templates/akash-network-awesome-akash-Llama-3.1-405B-FP8'
  },
  {
    id: 'Meta-Llama-3-2-3B-Instruct',
    name: 'Llama 3.2 3B',
    description: 'Fast model for quick responses',
    available: false,
    temperature: 0.6,
    top_p: 0.9,
    tokenLimit: 128000,
    parameters: '3B',
    architecture: 'Transformer',
    hf_repo: 'meta-llama/Llama-3.2-3B-Instruct',
    aboutContent: `Get instant, efficient performance with **Llama 3.2 3B**, a lightweight model ideal for fast, responsive interactions. Perfect for basic tasks, short conversations, and casual use cases, Llama 3.2 3B offers quick results without heavy compute demands.
  
  Accessible through AkashChat, this small but capable model is great for users seeking speed and simplicity on a decentralized platform.`,
    infoContent: `
  * ‚ö° Super-fast responses with minimal latency
  * üß† 128K-token context window for coherent exchanges
  * üåê Decentralized hosting ensures full control and lower costs
  * üîç Ideal for quick chats, FAQs, and lightweight workloads`,
    thumbnailId: 'llama-3',
    deployUrl: 'https://console.akash.network/templates/akash-network-awesome-akash-Llama-3.2-3B'
  },  
  {
    id: 'Meta-Llama-3-1-8B-Instruct-FP8',
    name: 'Llama 3.1 8B',
    description: 'Efficient model for basic tasks',
    available: false,
    temperature: 0.6,
    top_p: 0.9,
    tokenLimit: 128000,
    parameters: '8B',
    architecture: 'Transformer',
    hf_repo: 'meta-llama/Llama-3.1-8B-Instruct-FP8',
    aboutContent: `Discover the versatility of **Llama 3.1 8B**, a compact yet capable model perfect for daily tasks, chatbots, and lightweight reasoning. Available on AkashChat, Llama 3.1 8B offers a great balance between speed and capability without the need for large-scale compute.
  
  With FP8 optimization, it delivers faster inference and lower memory usage‚Äîideal for quick deployments and fast responses.`,
    infoContent: `
  * ‚ö° Fast, efficient model ready for instant interaction
  * üß† Supports extended conversations with a 128K-token window
  * üåê Cost-effective, decentralized deployment via Akash
  * üîç Best for lightweight applications, prototyping, and experimentation`,
    thumbnailId: 'llama-3',
    deployUrl: 'https://console.akash.network/templates/akash-network-awesome-akash-Llama-3.1-8B'
  },
  {
    id: 'mistral',
    name: 'Mistral-7B',
    description: 'Balanced model for general use',
    available: false,
    temperature: 0.7,
    top_p: 0.95,
    tokenLimit: 32768,
    parameters: '7B',
    architecture: 'Sliding Window Attention',
    hf_repo: 'mistral/Mistral-7B-v0.1',
    thumbnailId: 'mistral', 
    aboutContent: `Meet **Mistral-7B**, a lightweight, efficient language model known for its impressive performance across a wide range of general-purpose tasks. Now available on AkashChat, Mistral-7B is designed with sliding window attention for faster, more efficient handling of long sequences.
  
  It's perfect for developers and researchers looking for a compact yet capable model for real-world applications and experiments.`,
    infoContent: `
  * ‚ö° Highly efficient with sliding window attention
  * üß† Supports context-rich conversations up to 32K tokens
  * üåê Decentralized, low-cost deployment options
  * üîç Great for summarization, chatbots, and general-purpose AI tasks`,
    deployUrl: 'https://console.akash.network/templates/akash-network-awesome-akash-Mistral-7B'
  },
  {
    id: 'AkashGen',
    name: 'AkashGen',
    description: 'Generate images using AkashGen',
    available: true,
    temperature: 0.85,
    top_p: 1,
    tokenLimit: 12000,
    aboutContent: `AkashGen is a powerful image generation model that leverages the Akash Network for decentralized hosting. It allows you to generate images using a text prompt‚Äîno setup required.`,
    infoContent: `
* ‚ö° Instant access to AkashGen with no signup
* üåê Decentralized hosting for lower costs & full control
* üîç Great for image generation and creative content`,
    thumbnailId: 'akash-gen',
  }
];

// in case the `DEFAULT_MODEL` environment variable is not set or set to an unsupported model
export const fallbackModelID = 'Qwen3-235B-A22B-FP8';
export const defaultModel = process.env.DEFAULT_MODEL || fallbackModelID; 